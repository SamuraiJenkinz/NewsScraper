---
phase: 02-vertical-slice-validation
plan: 07
type: execute
wave: 3
depends_on: [02-01, 02-03, 02-04, 02-05, 02-06]
files_modified:
  - app/routers/runs.py
  - app/routers/__init__.py
  - app/main.py
autonomous: true

must_haves:
  truths:
    - "POST /api/runs/execute triggers end-to-end pipeline for one insurer"
    - "Pipeline: scrape -> classify -> store -> report -> email"
    - "Run status tracked in database throughout execution"
  artifacts:
    - path: "app/routers/runs.py"
      provides: "Run orchestration endpoints"
      exports: ["router"]
      min_lines: 100
  key_links:
    - from: "app/routers/runs.py"
      to: "app/services/scraper.py"
      via: "ApifyScraperService call"
      pattern: "ApifyScraperService"
    - from: "app/routers/runs.py"
      to: "app/services/classifier.py"
      via: "ClassificationService call"
      pattern: "ClassificationService"
    - from: "app/routers/runs.py"
      to: "app/services/emailer.py"
      via: "GraphEmailService call"
      pattern: "GraphEmailService"
    - from: "app/main.py"
      to: "app/routers/runs.py"
      via: "router registration"
      pattern: "include_router.*runs"
---

<objective>
Create run orchestration endpoint that executes the complete vertical slice pipeline.

Purpose: Single endpoint that validates the entire data flow from scraping through email delivery. Proves architecture before scaling to 897 insurers.

Output: /api/runs/execute endpoint that processes one insurer end-to-end with status tracking.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/02-vertical-slice-validation/02-RESEARCH.md

# All services
@app/services/scraper.py
@app/services/classifier.py
@app/services/emailer.py
@app/services/reporter.py

# Models
@app/models/run.py
@app/models/news_item.py
@app/models/insurer.py

# Config
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create runs router with execute endpoint</name>
  <files>app/routers/runs.py</files>
  <action>
Create the run orchestration router:

```python
# app/routers/runs.py
"""
Run orchestration endpoints for executing the intelligence pipeline.

Provides endpoints for triggering manual runs and querying run history.
The /execute endpoint runs the complete vertical slice pipeline.
"""
import logging
from datetime import datetime
from typing import Any

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from pydantic import BaseModel

from app.dependencies import get_db
from app.config import get_settings
from app.models.insurer import Insurer
from app.models.run import Run
from app.models.news_item import NewsItem
from app.schemas.run import RunCreate, RunRead, RunStatus, TriggerType
from app.services.scraper import ApifyScraperService
from app.services.classifier import ClassificationService
from app.services.reporter import ReportService
from app.services.emailer import GraphEmailService

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/runs", tags=["Runs"])


class ExecuteRequest(BaseModel):
    """Request body for execute endpoint."""
    category: str = "Health"
    insurer_id: int | None = None  # If None, picks first enabled insurer
    send_email: bool = True
    max_news_items: int = 5


class ExecuteResponse(BaseModel):
    """Response from execute endpoint."""
    run_id: int
    status: str
    insurers_processed: int
    items_found: int
    email_sent: bool
    message: str


@router.post("/execute", response_model=ExecuteResponse)
async def execute_run(
    request: ExecuteRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
) -> ExecuteResponse:
    """
    Execute a complete vertical slice pipeline run.

    This endpoint:
    1. Creates a run record
    2. Scrapes Google News for one insurer
    3. Classifies news with Azure OpenAI
    4. Stores results in database
    5. Generates HTML report
    6. Sends email (if configured)

    For vertical slice validation, processes ONE insurer to prove
    the end-to-end architecture before scaling.
    """
    settings = get_settings()

    # Create run record
    run = Run(
        category=request.category,
        trigger_type=TriggerType.MANUAL.value,
        status=RunStatus.RUNNING.value,
        started_at=datetime.utcnow(),
    )
    db.add(run)
    db.commit()
    db.refresh(run)

    logger.info(f"Starting run {run.id} for category {request.category}")

    try:
        # Get insurer to process
        if request.insurer_id:
            insurer = db.query(Insurer).filter(Insurer.id == request.insurer_id).first()
            if not insurer:
                raise HTTPException(status_code=404, detail="Insurer not found")
        else:
            # Pick first enabled insurer in category
            insurer = (
                db.query(Insurer)
                .filter(
                    Insurer.category == request.category,
                    Insurer.enabled == True,
                )
                .first()
            )
            if not insurer:
                raise HTTPException(
                    status_code=404,
                    detail=f"No enabled insurers found in {request.category} category",
                )

        logger.info(f"Processing insurer: {insurer.name} (ANS: {insurer.ans_code})")

        # Step 1: Scrape Google News
        scraper = ApifyScraperService()
        scraped_items = scraper.search_insurer(
            insurer_name=insurer.name,
            ans_code=insurer.ans_code,
            max_results=request.max_news_items,
        )
        logger.info(f"Scraped {len(scraped_items)} news items")

        if not scraped_items:
            logger.warning(f"No news found for {insurer.name}")
            # Create a "no news" record
            run.status = RunStatus.COMPLETED.value
            run.completed_at = datetime.utcnow()
            run.insurers_processed = 1
            run.items_found = 0
            db.commit()

            return ExecuteResponse(
                run_id=run.id,
                status="completed",
                insurers_processed=1,
                items_found=0,
                email_sent=False,
                message=f"No news found for {insurer.name}",
            )

        # Step 2: Classify with Azure OpenAI
        classifier = ClassificationService()
        news_items_data = []

        for scraped in scraped_items:
            # Classify single news item
            classification = classifier.classify_single_news(
                insurer_name=insurer.name,
                news_title=scraped.title,
                news_description=scraped.description,
            )

            # Store in database
            news_item = NewsItem(
                run_id=run.id,
                insurer_id=insurer.id,
                title=scraped.title,
                description=scraped.description,
                source_url=scraped.url,
                source_name=scraped.source,
                published_at=scraped.published_at,
                status=classification.status if classification else "Monitor",
                sentiment=classification.sentiment if classification else "neutral",
                summary=(
                    "\n".join(classification.summary_bullets)
                    if classification
                    else None
                ),
            )
            db.add(news_item)

            news_items_data.append({
                "title": scraped.title,
                "url": scraped.url,
                "source": scraped.source,
                "published_at": (
                    scraped.published_at.strftime("%Y-%m-%d")
                    if scraped.published_at
                    else None
                ),
                "sentiment": classification.sentiment if classification else "neutral",
                "summary_bullets": (
                    classification.summary_bullets
                    if classification
                    else ["Classificação indisponível"]
                ),
            })

        db.commit()
        logger.info(f"Stored {len(news_items_data)} classified news items")

        # Determine overall insurer status (use highest severity)
        status_order = ["Critical", "Watch", "Monitor", "Stable"]
        overall_status = "Stable"
        for item in news_items_data:
            item_status = "Monitor"  # Default
            for ni in db.query(NewsItem).filter(NewsItem.run_id == run.id).all():
                if ni.status:
                    if status_order.index(ni.status) < status_order.index(overall_status):
                        overall_status = ni.status

        # Step 3: Generate report
        reporter = ReportService()
        report_html = reporter.generate_report(
            category=request.category,
            insurers=[
                {
                    "name": insurer.name,
                    "ans_code": insurer.ans_code,
                    "status": overall_status,
                    "news_items": news_items_data,
                }
            ],
        )
        logger.info(f"Generated report ({len(report_html)} chars)")

        # Step 4: Send email (if configured and requested)
        email_sent = False
        if request.send_email and settings.is_graph_configured():
            emailer = GraphEmailService()
            email_result = await emailer.send_report_email(
                category=request.category,
                html_content=report_html,
                report_date=datetime.now().strftime("%Y-%m-%d"),
            )
            email_sent = email_result.get("status") == "ok"
            if email_sent:
                logger.info("Report email sent successfully")
            else:
                logger.warning(f"Email send failed: {email_result.get('message')}")
        else:
            logger.info("Email sending skipped (not configured or disabled)")

        # Update run record
        run.status = RunStatus.COMPLETED.value
        run.completed_at = datetime.utcnow()
        run.insurers_processed = 1
        run.items_found = len(news_items_data)
        db.commit()

        return ExecuteResponse(
            run_id=run.id,
            status="completed",
            insurers_processed=1,
            items_found=len(news_items_data),
            email_sent=email_sent,
            message=f"Successfully processed {insurer.name} with {len(news_items_data)} news items",
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Run {run.id} failed: {e}")
        run.status = RunStatus.FAILED.value
        run.completed_at = datetime.utcnow()
        run.error_message = str(e)
        db.commit()
        raise HTTPException(status_code=500, detail=str(e))


@router.get("", response_model=list[RunRead])
def list_runs(
    category: str | None = None,
    status: str | None = None,
    limit: int = 20,
    db: Session = Depends(get_db),
) -> list[RunRead]:
    """
    List recent runs with optional filtering.

    Args:
        category: Filter by category
        status: Filter by status
        limit: Maximum number of runs to return
    """
    query = db.query(Run).order_by(Run.started_at.desc())

    if category:
        query = query.filter(Run.category == category)
    if status:
        query = query.filter(Run.status == status)

    runs = query.limit(limit).all()
    return [RunRead.model_validate(run) for run in runs]


@router.get("/{run_id}", response_model=RunRead)
def get_run(
    run_id: int,
    db: Session = Depends(get_db),
) -> RunRead:
    """Get details of a specific run."""
    run = db.query(Run).filter(Run.id == run_id).first()
    if not run:
        raise HTTPException(status_code=404, detail="Run not found")
    return RunRead.model_validate(run)


@router.get("/{run_id}/news")
def get_run_news(
    run_id: int,
    db: Session = Depends(get_db),
) -> list[dict[str, Any]]:
    """Get all news items from a specific run."""
    run = db.query(Run).filter(Run.id == run_id).first()
    if not run:
        raise HTTPException(status_code=404, detail="Run not found")

    news_items = db.query(NewsItem).filter(NewsItem.run_id == run_id).all()

    return [
        {
            "id": item.id,
            "insurer_id": item.insurer_id,
            "insurer_name": item.insurer.name if item.insurer else None,
            "title": item.title,
            "description": item.description,
            "source_url": item.source_url,
            "source_name": item.source_name,
            "published_at": item.published_at,
            "status": item.status,
            "sentiment": item.sentiment,
            "summary": item.summary,
            "created_at": item.created_at,
        }
        for item in news_items
    ]
```
  </action>
  <verify>python -c "from app.routers.runs import router; print(f'Runs router has {len(router.routes)} routes')"</verify>
  <done>Runs router created with /execute, list, get, and get_news endpoints</done>
</task>

<task type="auto">
  <name>Task 2: Register runs router in main.py</name>
  <files>app/main.py, app/routers/__init__.py</files>
  <action>
Update app/main.py to include the runs router:

```python
# Add import
from app.routers import insurers, import_export, runs

# Add model imports to register tables
from app.models import insurer, run, news_item  # noqa: F401

# Register router (add after existing routers)
app.include_router(runs.router)
```

Update app/routers/__init__.py:
```python
from app.routers import insurers, import_export, runs
```

Ensure models are imported for table creation.
  </action>
  <verify>Start server and check /docs shows /api/runs endpoints</verify>
  <done>Runs router registered, all endpoints visible in OpenAPI docs</done>
</task>

</tasks>

<verification>
1. Routes exist: `python -c "from app.routers.runs import router; print([r.path for r in router.routes])"`
2. Server starts: `uvicorn app.main:app --reload` shows no import errors
3. OpenAPI docs: Visit http://localhost:8000/docs and see /api/runs endpoints
4. Database tables created: SQLite has runs and news_items tables
</verification>

<success_criteria>
- POST /api/runs/execute accepts category, insurer_id, send_email, max_news_items
- Execute endpoint creates Run record, scrapes news, classifies, stores, generates report, sends email
- Run status updated throughout execution (pending -> running -> completed/failed)
- GET /api/runs returns list of recent runs
- GET /api/runs/{id} returns run details
- GET /api/runs/{id}/news returns news items from run
- Error handling updates run with failed status and error_message
</success_criteria>

<output>
After completion, create `.planning/phases/02-vertical-slice-validation/02-07-SUMMARY.md`
</output>
