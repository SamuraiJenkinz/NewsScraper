---
phase: 02-vertical-slice-validation
plan: 03
type: execute
wave: 2
depends_on: [02-02]
files_modified:
  - app/services/scraper.py
autonomous: true

must_haves:
  truths:
    - "Service connects to Apify and calls Google News scraper actor"
    - "Search queries combine insurer name and ANS code for accuracy"
    - "Results include title, description, URL, source, and publication date"
  artifacts:
    - path: "app/services/scraper.py"
      provides: "ApifyScraperService class wrapping Apify client"
      exports: ["ApifyScraperService"]
      min_lines: 50
  key_links:
    - from: "app/services/scraper.py"
      to: "app/config.py"
      via: "get_settings() for APIFY_TOKEN"
      pattern: "get_settings\\(\\)"
---

<objective>
Create Apify scraper service to fetch Google News results for insurers.

Purpose: Enables automated news collection from Google News using Apify's managed infrastructure with rate limiting and proxy rotation.

Output: ApifyScraperService class that searches for insurer news and returns structured results.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/02-vertical-slice-validation/02-RESEARCH.md

# Config for API token
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ApifyScraperService class</name>
  <files>app/services/scraper.py</files>
  <action>
Create Apify scraper service based on research pattern:

```python
# app/services/scraper.py
"""
Apify-based web scraper service for Google News.

Uses the lhotanova/google-news-scraper actor to fetch news
about Brazilian insurers with proper rate limiting.
"""
import logging
from typing import Any
from datetime import datetime

from apify_client import ApifyClient

from app.config import get_settings

logger = logging.getLogger(__name__)


class ScrapedNewsItem:
    """Represents a scraped news item from Google News."""

    def __init__(
        self,
        title: str,
        description: str | None = None,
        url: str | None = None,
        source: str | None = None,
        published_at: datetime | None = None,
        raw_data: dict | None = None,
    ):
        self.title = title
        self.description = description
        self.url = url
        self.source = source
        self.published_at = published_at
        self.raw_data = raw_data or {}

    def __repr__(self) -> str:
        return f"ScrapedNewsItem(title={self.title[:50]}..., source={self.source})"


class ApifyScraperService:
    """
    Service for scraping Google News using Apify actors.

    Wraps the Apify client and provides methods for searching
    news about specific insurers using their name and ANS code.
    """

    # Google News scraper actor from Apify Store
    GOOGLE_NEWS_ACTOR = "lhotanova/google-news-scraper"

    def __init__(self):
        settings = get_settings()
        if not settings.is_apify_configured():
            logger.warning("Apify token not configured - scraping will fail")
            self.client = None
        else:
            self.client = ApifyClient(settings.apify_token)

    def search_google_news(
        self,
        query: str,
        language: str = "pt",
        country: str = "BR",
        max_results: int = 10,
        time_filter: str = "7d",
        timeout_secs: int = 300,
    ) -> list[ScrapedNewsItem]:
        """
        Search Google News for a query string.

        Args:
            query: Search query (supports quoted phrases and OR)
            language: Language code (pt for Portuguese)
            country: Country code (BR for Brazil)
            max_results: Maximum number of results to return
            time_filter: Time range (7d, 1m, 1y, etc.)
            timeout_secs: Actor execution timeout

        Returns:
            List of ScrapedNewsItem objects
        """
        if not self.client:
            logger.error("Apify client not initialized - check APIFY_TOKEN")
            return []

        run_input = {
            "queries": query,
            "language": language,
            "country": country,
            "maxItems": max_results,
            "timeRange": time_filter,
        }

        logger.info(f"Starting Google News search: {query[:100]}...")

        try:
            # Run actor and wait for completion
            run = self.client.actor(self.GOOGLE_NEWS_ACTOR).call(
                run_input=run_input,
                timeout_secs=timeout_secs,
            )

            # Retrieve results from default dataset
            items = list(self.client.dataset(run["defaultDatasetId"]).iterate_items())
            logger.info(f"Found {len(items)} news items for query")

            return self._parse_results(items)

        except Exception as e:
            logger.error(f"Apify scraping failed: {e}")
            return []

    def search_insurer(
        self,
        insurer_name: str,
        ans_code: str,
        max_results: int = 10,
    ) -> list[ScrapedNewsItem]:
        """
        Search for news about a specific insurer.

        Combines insurer name and ANS code in search query
        for better accuracy and relevance.

        Args:
            insurer_name: Full insurer name
            ans_code: ANS registration code
            max_results: Maximum results to return

        Returns:
            List of ScrapedNewsItem objects
        """
        # Build query with name and ANS code for better matching
        # Use quotes for exact phrase matching on name
        query = f'"{insurer_name}" OR "ANS {ans_code}"'

        return self.search_google_news(
            query=query,
            max_results=max_results,
        )

    def _parse_results(self, items: list[dict[str, Any]]) -> list[ScrapedNewsItem]:
        """
        Parse raw Apify results into ScrapedNewsItem objects.

        Handles various field names and missing data gracefully.
        """
        results = []

        for item in items:
            try:
                # Extract publication date if available
                published_at = None
                date_str = item.get("publishedAt") or item.get("date")
                if date_str:
                    try:
                        # Handle various date formats
                        if isinstance(date_str, str):
                            # Try ISO format first
                            published_at = datetime.fromisoformat(
                                date_str.replace("Z", "+00:00")
                            )
                    except ValueError:
                        logger.debug(f"Could not parse date: {date_str}")

                news_item = ScrapedNewsItem(
                    title=item.get("title", "No title"),
                    description=item.get("description") or item.get("snippet"),
                    url=item.get("link") or item.get("url"),
                    source=item.get("source") or item.get("publisher"),
                    published_at=published_at,
                    raw_data=item,
                )
                results.append(news_item)

            except Exception as e:
                logger.warning(f"Failed to parse news item: {e}")
                continue

        return results

    def health_check(self) -> dict[str, Any]:
        """
        Check Apify service connectivity.

        Returns dict with status and any error message.
        """
        if not self.client:
            return {"status": "error", "message": "APIFY_TOKEN not configured"}

        try:
            # Try to get user info as a connectivity test
            user = self.client.user().get()
            return {
                "status": "ok",
                "username": user.get("username"),
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}
```
  </action>
  <verify>python -c "from app.services.scraper import ApifyScraperService; s = ApifyScraperService(); print('Scraper service OK')"</verify>
  <done>ApifyScraperService class created with search_google_news, search_insurer, and health_check methods</done>
</task>

<task type="auto">
  <name>Task 2: Update services __init__.py</name>
  <files>app/services/__init__.py</files>
  <action>
Export the scraper service:

```python
# app/services/__init__.py
from app.services.excel_service import parse_excel_upload, generate_excel_export
from app.services.scraper import ApifyScraperService
```
  </action>
  <verify>python -c "from app.services import ApifyScraperService; print('Export OK')"</verify>
  <done>ApifyScraperService exported from services package</done>
</task>

</tasks>

<verification>
1. Service imports: `python -c "from app.services.scraper import ApifyScraperService, ScrapedNewsItem"`
2. Instance creates without error (even without token): `python -c "from app.services.scraper import ApifyScraperService; s = ApifyScraperService(); print(s.health_check())"`
3. Config integration: Service reads APIFY_TOKEN from settings
4. Health check returns appropriate status based on configuration
</verification>

<success_criteria>
- ApifyScraperService connects to Apify using token from Settings
- search_google_news() accepts query, language, country, max_results, time_filter
- search_insurer() builds query from name and ANS code
- Results parsed into ScrapedNewsItem objects with title, description, url, source, published_at
- Graceful error handling when token missing or actor fails
- health_check() returns service status
</success_criteria>

<output>
After completion, create `.planning/phases/02-vertical-slice-validation/02-03-SUMMARY.md`
</output>
