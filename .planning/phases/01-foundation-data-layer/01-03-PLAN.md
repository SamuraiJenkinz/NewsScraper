---
phase: 01-foundation-data-layer
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - app/services/__init__.py
  - app/services/excel_service.py
  - app/routers/import_export.py
  - app/main.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Admin can upload Excel file and see preview of parsed data"
    - "Preview shows validation errors for invalid rows"
    - "Preview identifies duplicate ANS codes"
    - "Admin can commit previewed data to database"
    - "System validates required fields before commit"
  artifacts:
    - path: "app/services/excel_service.py"
      provides: "Excel parsing and validation logic"
      exports: ["parse_excel_insurers", "generate_excel_export"]
    - path: "app/routers/import_export.py"
      provides: "Import preview and commit endpoints"
      exports: ["router"]
  key_links:
    - from: "app/routers/import_export.py"
      to: "app/services/excel_service.py"
      via: "parse_excel_insurers call"
      pattern: "parse_excel_insurers\\("
    - from: "app/services/excel_service.py"
      to: "app/schemas/insurer.py"
      via: "Pydantic validation"
      pattern: "InsurerCreate\\("
---

<objective>
Implement Excel import with preview-before-commit workflow and validation.

Purpose: Enable bulk insurer data import from Excel with preview step - fulfilling DATA-04 (upload Excel), DATA-05 (preview before commit), DATA-07 (validate required fields), DATA-08 (reject duplicates).

Output: Working import workflow with /api/import/preview and /api/import/commit endpoints.
</objective>

<execution_context>
@C:\Users\taylo\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\taylo\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@C:\BrasilIntel\.planning\PROJECT.md
@C:\BrasilIntel\.planning\phases\01-foundation-data-layer\01-RESEARCH.md
@C:\BrasilIntel\.planning\phases\01-foundation-data-layer\01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Excel parsing service with column normalization</name>
  <files>
    app/services/excel_service.py
  </files>
  <action>
    Create app/services/excel_service.py following research Pitfall 2 and Pitfall 6:

    1. Define COLUMN_MAP for flexible column matching:
       ```python
       COLUMN_MAP = {
           'ans_code': ['ans_code', 'anscode', 'ans code', 'ans', 'code', 'ans code'],
           'name': ['insurer_name', 'name', 'company_name', 'insurer name', 'razao_social'],
           'cnpj': ['company_registration_number', 'cnpj', 'registration'],
           'category': ['product', 'category', 'type', 'produto'],
           'market_master': ['market_master', 'market master', 'marketmaster']
       }
       ```

    2. Create normalize_columns(df) function:
       - Strip whitespace and lowercase all column names
       - Replace spaces with underscores
       - Map columns using COLUMN_MAP variants
       - Return df with normalized columns

    3. Create parse_excel_insurers(file) function:
       ```python
       def parse_excel_insurers(file) -> tuple[list[dict], list[dict]]:
           """
           Parse Excel file and validate rows.
           Returns (validated_rows, errors)
           """
           # Read with explicit missing value handling (Pitfall 2)
           df = pd.read_excel(
               file,
               engine='openpyxl',
               na_values=['', 'NA', 'N/A', 'null', 'Nil', '?', 'nan', 'NaN'],
               keep_default_na=True
           )

           # Normalize columns
           df = normalize_columns(df)

           # Fill missing optional fields
           df = df.fillna({
               'cnpj': '',
               'market_master': '',
               'status': ''
           })

           validated = []
           errors = []

           for idx, row in df.iterrows():
               row_num = idx + 2  # Excel row (1-indexed + header)
               try:
                   # Convert ans_code to string, handle floats
                   ans_code = str(int(row['ans_code'])) if pd.notna(row['ans_code']) else ''
                   ans_code = ans_code.zfill(6)  # Pad to 6 digits

                   insurer = InsurerCreate(
                       ans_code=ans_code,
                       name=str(row['name']).strip() if pd.notna(row['name']) else '',
                       cnpj=str(row.get('cnpj', '')).strip(),
                       category=str(row['category']).strip() if pd.notna(row['category']) else '',
                       market_master=str(row.get('market_master', '')).strip()
                   )
                   validated.append(insurer.model_dump())
               except Exception as e:
                   errors.append({
                       'row': row_num,
                       'ans_code': str(row.get('ans_code', 'N/A')),
                       'error': str(e)
                   })

           return validated, errors
       ```

    4. Handle category normalization:
       - Map common variants: "Saude" -> "Health", "Odontologico" -> "Dental", "Vida" -> "Group Life"
  </action>
  <verify>
    `python -c "from app.services.excel_service import parse_excel_insurers; print('OK')"`
  </verify>
  <done>Excel parsing service created with column normalization and missing data handling</done>
</task>

<task type="auto">
  <name>Task 2: Create import router with preview-before-commit workflow</name>
  <files>
    app/routers/import_export.py
  </files>
  <action>
    Create app/routers/import_export.py following research Pattern 3:

    1. Create preview session storage (in-memory for MVP):
       ```python
       from typing import Dict
       import uuid
       from datetime import datetime, timedelta

       # Session storage with TTL
       preview_sessions: Dict[str, dict] = {}
       SESSION_TTL_MINUTES = 30
       ```

    2. Create POST "/api/import/preview" endpoint:
       ```python
       @router.post("/preview")
       async def preview_import(
           file: UploadFile,
           db: Session = Depends(get_db)
       ):
           # Validate file type
           if not file.filename.endswith(('.xlsx', '.xls')):
               raise HTTPException(400, "File must be Excel format (.xlsx or .xls)")

           # Parse Excel
           validated, errors = parse_excel_insurers(file.file)

           # Check for duplicates in uploaded data
           seen_codes = {}
           internal_duplicates = []
           for item in validated:
               code = item['ans_code']
               if code in seen_codes:
                   internal_duplicates.append({
                       'ans_code': code,
                       'first_row': seen_codes[code],
                       'duplicate_row': validated.index(item) + 2
                   })
               else:
                   seen_codes[code] = validated.index(item) + 2

           # Check for existing ANS codes in database
           existing_codes = {r[0] for r in db.query(Insurer.ans_code).all()}
           db_duplicates = [
               {'ans_code': v['ans_code'], 'action': 'will_update'}
               for v in validated if v['ans_code'] in existing_codes
           ]

           # Store session
           session_id = str(uuid.uuid4())
           preview_sessions[session_id] = {
               'data': validated,
               'created_at': datetime.utcnow(),
               'existing_codes': list(existing_codes)
           }

           return {
               'session_id': session_id,
               'total_rows': len(validated),
               'valid_rows': len(validated) - len(internal_duplicates),
               'errors': errors,
               'internal_duplicates': internal_duplicates,
               'existing_in_db': db_duplicates,
               'preview': validated[:20],  # First 20 for display
               'expires_in_minutes': SESSION_TTL_MINUTES
           }
       ```

    3. Create POST "/api/import/commit/{session_id}" endpoint:
       ```python
       @router.post("/commit/{session_id}")
       async def commit_import(
           session_id: str,
           mode: str = "merge",  # "merge" or "replace"
           db: Session = Depends(get_db)
       ):
           # Validate session exists
           if session_id not in preview_sessions:
               raise HTTPException(404, "Preview session not found or expired")

           session = preview_sessions[session_id]

           # Check session age
           if datetime.utcnow() - session['created_at'] > timedelta(minutes=SESSION_TTL_MINUTES):
               del preview_sessions[session_id]
               raise HTTPException(410, "Preview session expired")

           validated_data = session['data']
           existing_codes = set(session['existing_codes'])

           try:
               created = 0
               updated = 0

               for item in validated_data:
                   if item['ans_code'] in existing_codes:
                       # Update existing
                       db.query(Insurer).filter(
                           Insurer.ans_code == item['ans_code']
                       ).update(item)
                       updated += 1
                   else:
                       # Create new
                       db.add(Insurer(**item, enabled=True))
                       created += 1

               db.commit()

               # Clean up session
               del preview_sessions[session_id]

               return {
                   'status': 'success',
                   'created': created,
                   'updated': updated,
                   'total': created + updated
               }
           except Exception as e:
               db.rollback()
               raise HTTPException(500, f"Import failed: {str(e)}")
       ```
  </action>
  <verify>
    `python -c "from app.routers.import_export import router; print(router.prefix)"`
  </verify>
  <done>Import router with preview and commit endpoints created</done>
</task>

<task type="auto">
  <name>Task 3: Register router and test import workflow</name>
  <files>
    app/main.py
  </files>
  <action>
    Update app/main.py:
    1. Import router: `from app.routers import import_export`
    2. Register: `app.include_router(import_export.router)`

    Test the import workflow:

    1. Create a test Excel file with a few insurers (or use the reference ByCat3.xlsx)

    2. Test preview:
       ```
       curl -X POST http://localhost:8000/api/import/preview \
         -F "file=@test_insurers.xlsx"
       ```
       Should return session_id, total_rows, errors, preview

    3. Test commit with session_id from preview:
       ```
       curl -X POST "http://localhost:8000/api/import/commit/{session_id}"
       ```
       Should return created/updated counts

    4. Verify data:
       ```
       curl http://localhost:8000/api/insurers
       ```
       Should show imported insurers

    5. Test validation errors - create Excel with:
       - Missing ANS code (should error)
       - Invalid category (should error)
       - Duplicate ANS code in same file (should warn)

    6. Test re-import same file:
       - Preview should show "existing_in_db" entries
       - Commit should update existing records
  </action>
  <verify>
    Complete workflow test as described
  </verify>
  <done>Import workflow tested end-to-end with preview and commit</done>
</task>

</tasks>

<verification>
1. POST /api/import/preview accepts Excel file and returns parsed data
2. Preview response includes validation errors for invalid rows (DATA-07)
3. Preview response identifies duplicate ANS codes in file and database (DATA-08)
4. POST /api/import/commit/{session_id} imports data to database
5. Re-import updates existing records (merge mode)
6. Invalid session_id returns 404
7. Expired session returns 410
</verification>

<success_criteria>
- Admin can upload Excel file (DATA-04)
- Admin sees preview before committing (DATA-05)
- System validates required fields (ANS Code, Name, Category) (DATA-07)
- System identifies and handles duplicate ANS codes (DATA-08)
- Commit imports valid rows to database
- Invalid rows shown with clear error messages
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-data-layer/01-03-SUMMARY.md`
</output>
