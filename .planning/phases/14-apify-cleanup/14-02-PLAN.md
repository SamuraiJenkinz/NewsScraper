---
phase: 14-apify-cleanup
plan: 02
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - app/config.py
  - app/routers/runs.py
  - app/routers/admin.py
  - app/templates/admin/settings.html
  - .env.example
  - docs/USER_GUIDE.md
autonomous: true

must_haves:
  truths:
    - "The pipeline collection step has no conditional branch for Apify — only the Factiva path exists"
    - ".env.example contains MMC Core API variables and APIFY_TOKEN is absent"
    - "FastAPI app boots without import errors after all Apify cleanup"
    - "No Apify remnants exist anywhere in the application code"
  artifacts:
    - path: "app/config.py"
      provides: "Settings class with no Apify fields or scraping config"
    - path: "app/routers/runs.py"
      provides: "Pipeline router with Factiva-only execution path"
    - path: ".env.example"
      provides: "Environment template with MMC Core API config, no APIFY_TOKEN"
    - path: "docs/USER_GUIDE.md"
      provides: "Updated documentation reflecting Apify removal"
  key_links:
    - from: "app/routers/runs.py"
      to: "app/collectors/factiva.py"
      via: "FactivaCollector import"
      pattern: "from app\\.collectors\\.factiva import FactivaCollector"
    - from: "app/config.py"
      to: ".env.example"
      via: "Environment variable mapping"
      pattern: "mmc_api"
---

<objective>
Clean remaining Apify references from config, pipeline, admin UI, environment template, and documentation. Remove legacy pipeline functions that used ScraperService. Remove orphaned scraping settings from config and admin display. Validate the app boots cleanly and no Apify remnants exist.

Purpose: Complete the Apify removal by scrubbing all remaining references and validating the entire cleanup is clean.
Output: 6 files modified, legacy pipeline functions removed, scraping config removed, env template updated, docs updated, boot validated.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-apify-cleanup/14-RESEARCH.md
@.planning/phases/14-apify-cleanup/14-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Remove Apify config, scraping settings, and legacy pipeline functions</name>
  <files>
    app/config.py
    app/routers/runs.py
  </files>
  <action>
    **app/config.py** — Remove ALL Apify and legacy scraping configuration:

    1. Remove the Apify field and comment (around line 42-43):
       ```python
       # Apify (for web scraping)
       apify_token: str = ""
       ```

    2. Remove the batch processing / scraping settings block (lines 63-72):
       ```python
       # Batch processing settings (NEWS-07)
       batch_size: int = 30
       batch_delay_seconds: float = 2.0
       max_concurrent_sources: int = 3
       scrape_timeout_seconds: int = 60
       scrape_max_results: int = 10

       # Source-specific timeouts (website crawlers need longer)
       source_timeout_valor: int = 120
       source_timeout_cqcs: int = 120
       ```

    3. Remove the relevance scoring settings block (lines 74-77). These fields are tied to RelevanceScorer which is deleted in 14-01:
       ```python
       # Relevance scoring settings (NEWS-10)
       use_ai_relevance_scoring: bool = True  # Enable AI relevance pre-filter
       relevance_keyword_threshold: int = 20  # Items below this skip AI scoring
       relevance_ai_batch_size: int = 10  # Items per AI scoring request
       ```

    4. Remove the `is_apify_configured()` method (lines 173-175):
       ```python
       def is_apify_configured(self) -> bool:
           """Check if Apify is configured."""
           return bool(self.apify_token)
       ```

    5. Remove the `get_source_timeout()` method (lines 177-191):
       ```python
       def get_source_timeout(self, source_name: str) -> int:
           ...
       ```

    Leave all other config fields and methods untouched. The MMC Core API, Azure OpenAI, Graph, scheduler, report, and admin settings remain.

    **app/routers/runs.py** — Remove legacy Apify pipeline functions and any ScraperService import:

    1. Remove any `ScraperService` import at the top of the file if present (e.g. `from app.services.scraper import ScraperService`). After 14-01 deletes scraper.py, any remaining import would cause an ImportError. Check lines 1-30 and remove if found.

    2. Delete the entire `_execute_single_insurer_run()` function (lines 364-470). This is Phase 2 legacy code that creates `ScraperService()` — dead code since Factiva replaced Apify.

    3. Delete the entire `_execute_category_run()` function (lines 473-564). This is Phase 3 legacy code that creates `ScraperService()` — dead code since Factiva replaced Apify.

    4. Update the module docstring (lines 1-8) to remove references to "Single insurer processing (Phase 2 compatibility)" and "Full category processing (Phase 3 scale)". The docstring should reflect Factiva-only pipeline:
       ```python
       """
       Run orchestration router for BrasilIntel.

       Provides endpoints to execute the Factiva news collection pipeline:
       - Category-based news collection via Factiva/Dow Jones
       - AI classification and insurer matching
       - Professional report generation with equity data
       - Critical alerts and PDF delivery
       """
       ```

    DO NOT modify `_execute_factiva_pipeline()` or `_generate_and_send_report()` — these are the active Factiva pipeline.
    DO NOT remove any imports at the top of runs.py EXCEPT for ScraperService-related imports (step 1 above) — the remaining imports are all used by the active pipeline.
  </action>
  <verify>
    - `grep -n "ScraperService\|_execute_single_insurer\|_execute_category_run\|apify" app/routers/runs.py` — no matches
    - `grep -n "from app.services.scraper import\|from app.services import.*ScraperService" app/routers/runs.py` — no matches (import removed or confirmed absent)
    - `grep -n "apify_token\|is_apify_configured\|get_source_timeout\|batch_size\|scrape_timeout\|source_timeout" app/config.py` — no matches
    - `grep -n "use_ai_relevance_scoring\|relevance_keyword_threshold\|relevance_ai_batch_size\|Relevance scoring" app/config.py` — no matches (relevance scoring block removed)
    - `grep -n "FactivaCollector\|_execute_factiva_pipeline" app/routers/runs.py` — still present (active pipeline preserved)
    - `grep -n "mmc_api\|is_mmc" app/config.py` — still present (enterprise config preserved)
  </verify>
  <done>config.py has no Apify fields, legacy scraping settings, or relevance scoring settings. runs.py has no ScraperService import and no legacy Apify pipeline functions. Only Factiva pipeline path remains.</done>
</task>

<task type="auto">
  <name>Task 2: Clean admin UI, update env template and docs, validate boot</name>
  <files>
    app/routers/admin.py
    app/templates/admin/settings.html
    .env.example
    docs/USER_GUIDE.md
  </files>
  <action>
    **app/routers/admin.py** — Remove the scraping config section from the settings route (around lines 1141-1148):
    ```python
    # Scraping configuration (ADMN-15)
    scraping_config = {
        "batch_size": settings.batch_size,
        "batch_delay_seconds": settings.batch_delay_seconds,
        "max_concurrent_sources": settings.max_concurrent_sources,
        "scrape_timeout_seconds": settings.scrape_timeout_seconds,
        "scrape_max_results": settings.scrape_max_results,
    }
    ```
    Also remove `scraping_config` from the template context dict passed to the template (find where it's passed to `templates.TemplateResponse` and remove the key).

    **app/templates/admin/settings.html** — Remove the "Scraping Configuration" card (the `<div class="card mb-4">` block that displays batch_size, batch_delay, max_concurrent_sources, scrape_timeout, scrape_max_results). This is approximately lines 33-58. The card has a header and a `<dl>` with 5 definition list items plus a `<small>` with env var names.

    **`.env.example`** — Remove the Apify Configuration section (lines 49-53):
    ```
    # ============================================
    # Apify Configuration (for web scraping)
    # ============================================
    # Your Apify API token from https://console.apify.com/settings/integrations
    APIFY_TOKEN=your-apify-token-here
    ```
    Leave the Report Recipients section that follows immediately after. The MMC Core API section (lines 103-124) stays as-is.

    **`docs/USER_GUIDE.md`** — Update the note at line 702 from:
    ```
    > **Note:** In v1.0, news was collected from 7 Apify-based sources (Google News, Valor Economico, InfoMoney, CQCS, ANS, Estadao, RSS). v1.1 replaced all sources with Factiva via MMC Core API. Legacy Apify source code remains in the codebase but is not used in the active pipeline.
    ```
    To:
    ```
    > **Note:** In v1.0, news was collected from 7 Apify-based sources (Google News, Valor Economico, InfoMoney, CQCS, ANS, Estadao, RSS). v1.1 replaced all sources with Factiva via MMC Core API. All legacy Apify infrastructure was removed — Factiva is the sole news collection mechanism.
    ```

    **Boot Validation** — After all changes, run:
    ```bash
    cd C:\BrasilIntel && python -c "from app.main import app; print('FastAPI app boots OK')"
    ```
    This verifies no import errors from removed modules.

    **Remnant Grep** — Run a codebase-wide search for Apify remnants in application code:
    ```bash
    grep -ri "apify\|scraper_service\|ScraperService\|batch_processor\|BatchProcessor\|SourceRegistry\|ScrapedNewsItem" app/ --include="*.py" --include="*.html"
    ```
    Expect zero matches in application code (planning docs may still reference Apify historically — that is fine).
  </action>
  <verify>
    - `grep -ri "APIFY_TOKEN\|apify" .env.example` — no matches
    - `grep -ri "scraping_config\|max_concurrent_sources\|scrape_timeout\|scrape_max_results" app/routers/admin.py` — no matches
    - `grep -ri "Scrape Timeout\|Max Concurrent Sources\|Max Results/Insurer" app/templates/admin/settings.html` — no matches
    - `grep "Legacy Apify source code remains" docs/USER_GUIDE.md` — no match (updated)
    - `python -c "from app.main import app; print('OK')"` — prints OK (boot succeeds)
    - `grep -ri "apify\|ScraperService\|BatchProcessor\|SourceRegistry\|ScrapedNewsItem" app/ --include="*.py" --include="*.html"` — no matches in app code
  </verify>
  <done>Admin settings page no longer shows scraping config. .env.example has no APIFY_TOKEN. USER_GUIDE.md reflects Apify removal. FastAPI app boots without errors. No Apify remnants in application code.</done>
</task>

</tasks>

<verification>
After both tasks, verify ALL phase success criteria:

1. **CLNP-01**: `ls app/services/sources/` fails — all source class files removed (done in 14-01)
2. **CLNP-02**: `grep -i "apify-client\|feedparser" requirements.txt` — no matches (done in 14-01)
3. **CLNP-03**: `grep -n "_execute_single_insurer\|_execute_category_run\|ScraperService" app/routers/runs.py` — no matches. Only `_execute_factiva_pipeline` remains.
4. **CLNP-04**: `grep -i "APIFY_TOKEN\|apify" .env.example` — no matches. MMC Core API section present.
5. **Boot check**: `python -c "from app.main import app; print('OK')"` succeeds
6. **Full remnant scan**: `grep -ri "apify\|ScraperService\|BatchProcessor\|SourceRegistry\|RelevanceScorer\|relevance_ai_batch" app/ --include="*.py"` — no matches
</verification>

<success_criteria>
- app/config.py has no Apify/scraping fields (apify_token, batch_size, scrape_timeout, source_timeout, relevance scoring block removed)
- app/routers/runs.py has no legacy pipeline functions (Factiva-only path)
- Admin settings page shows no scraping config section
- .env.example has no APIFY_TOKEN and retains MMC Core API section
- docs/USER_GUIDE.md reflects Apify removal completion
- FastAPI app boots successfully
- Zero Apify remnants in application code (app/**/*.py, app/**/*.html)
</success_criteria>

<output>
After completion, create `.planning/phases/14-apify-cleanup/14-02-SUMMARY.md`
</output>
