---
phase: 11-insurer-matching-pipeline
plan: 03
type: execute
wave: 3
depends_on: ["11-01", "11-02"]
files_modified:
  - app/routers/runs.py
  - app/services/insurer_matcher.py
autonomous: true

must_haves:
  truths:
    - "The pipeline collection step invokes FactivaCollector — no Apify ScraperService code path remains in the active flow"
    - "After a pipeline run, each stored article has at least one insurer_id assignment"
    - "Articles are matched via deterministic path first, then AI for ambiguous cases"
    - "Unmatched articles (no insurer found by either method) are still stored using a sentinel 'General News' insurer"
    - "Multi-insurer articles create one NewsItem row per matched insurer (same title, different insurer_id)"
    - "The scraper health endpoint is removed or replaced"
  artifacts:
    - path: "app/routers/runs.py"
      provides: "Refactored pipeline using Factiva collection, deduplication, and insurer matching"
      contains: "FactivaCollector"
    - path: "app/services/insurer_matcher.py"
      provides: "Updated InsurerMatcher with AI fallback wired in"
      contains: "AIInsurerMatcher"
  key_links:
    - from: "app/routers/runs.py"
      to: "app/collectors/factiva.py"
      via: "FactivaCollector.collect() replaces ScraperService"
      pattern: "FactivaCollector"
    - from: "app/routers/runs.py"
      to: "app/services/deduplicator.py"
      via: "ArticleDeduplicator.deduplicate() after collection"
      pattern: "ArticleDeduplicator"
    - from: "app/routers/runs.py"
      to: "app/services/insurer_matcher.py"
      via: "InsurerMatcher.match_batch() assigns articles to insurers"
      pattern: "InsurerMatcher"
    - from: "app/services/insurer_matcher.py"
      to: "app/services/ai_matcher.py"
      via: "InsurerMatcher calls AIInsurerMatcher for ambiguous articles"
      pattern: "AIInsurerMatcher"
---

<objective>
Wire the complete Factiva pipeline: replace Apify collection with FactivaCollector, integrate deduplication and insurer matching, and handle multi-insurer/unmatched articles.

Purpose: This is the pipeline switchover plan. After this, the system collects Brazilian insurance news from Factiva in batch, deduplicates, matches to insurers deterministically and via AI, then stores and classifies. Apify is no longer called. This satisfies FACT-04 (insurer matching) and FACT-06 (Factiva as sole source).

Output: Rewritten `app/routers/runs.py` pipeline flow + updated InsurerMatcher with AI fallback.
</objective>

<execution_context>
@C:\Users\taylo\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\taylo\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-insurer-matching-pipeline/11-RESEARCH.md
@.planning/phases/11-insurer-matching-pipeline/11-01-SUMMARY.md
@.planning/phases/11-insurer-matching-pipeline/11-02-SUMMARY.md
@app/routers/runs.py
@app/collectors/factiva.py
@app/services/deduplicator.py
@app/services/insurer_matcher.py
@app/services/ai_matcher.py
@app/services/classifier.py
@app/models/insurer.py
@app/models/news_item.py
@app/models/run.py
@app/models/factiva_config.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire AI fallback into InsurerMatcher</name>
  <files>app/services/insurer_matcher.py</files>
  <action>
Update `app/services/insurer_matcher.py` to integrate AIInsurerMatcher as a fallback for ambiguous articles.

**Changes to InsurerMatcher class:**

1. **Add AIInsurerMatcher integration to constructor:**
   ```python
   from app.services.ai_matcher import AIInsurerMatcher
   ```
   - Create `self.ai_matcher = AIInsurerMatcher()` in `__init__`
   - Add `self.ai_enabled = self.ai_matcher.is_configured()` flag
   - Log whether AI matching is available: `self.logger.info("insurer_matcher_init", ai_enabled=self.ai_enabled)`

2. **Update match_article to use AI fallback:**
   The current logic returns "unmatched" for 0 or >3 deterministic matches. Change it to:
   - If 0 matches AND AI enabled: call `self.ai_matcher.ai_match(article, insurers)` and return its result
   - If >3 matches AND AI enabled: call `self.ai_matcher.ai_match(article, insurers)` and return its result (too many matches = ambiguous)
   - If 0 or >3 matches AND AI NOT enabled: return unmatched MatchResult as before
   - Keep the existing deterministic_single (1 match) and deterministic_multi (2-3 matches) paths unchanged

3. **Update match_batch logging:**
   After processing all articles, log summary stats:
   ```python
   self.logger.info(
       "match_batch_complete",
       total=len(articles),
       deterministic_single=count_single,
       deterministic_multi=count_multi,
       ai_disambiguation=count_ai,
       unmatched=count_unmatched,
   )
   ```

Do NOT change the file structure, class name, or public API. Only add the AI fallback wiring and enhanced logging.
  </action>
  <verify>
`python -c "from app.services.insurer_matcher import InsurerMatcher; m = InsurerMatcher(); print(f'AI enabled: {m.ai_enabled}')"` prints without error.
  </verify>
  <done>InsurerMatcher.match_article falls back to AIInsurerMatcher for ambiguous cases (0 or >3 deterministic matches). Match stats are logged per batch.</done>
</task>

<task type="auto">
  <name>Task 2: Rewrite pipeline in runs.py to use Factiva + matching</name>
  <files>app/routers/runs.py</files>
  <action>
Rewrite the pipeline functions in `app/routers/runs.py` to replace the per-insurer Apify scraping flow with the batch Factiva collection + matching flow.

**Step 1: Update imports.**
Remove:
```python
from app.services.scraper import ApifyScraperService, ScraperService
```
Add:
```python
from app.collectors.factiva import FactivaCollector
from app.services.deduplicator import ArticleDeduplicator
from app.services.insurer_matcher import InsurerMatcher
from app.models.factiva_config import FactivaConfig
```
Keep all existing imports (ClassificationService, GraphEmailService, ReportService, CriticalAlertService, etc.).

**Step 2: Create new private function `_execute_factiva_pipeline`.**
This replaces both `_execute_single_insurer_run` and `_execute_category_run` with a unified Factiva-based flow. The new pipeline does NOT iterate per-insurer — it collects all articles in batch, then matches.

```python
async def _execute_factiva_pipeline(
    request: ExecuteRequest,
    run: Run,
    db: Session,
) -> ExecuteResponse:
```

**Pipeline steps:**

1. **Load FactivaConfig from DB:**
   ```python
   factiva_config = db.query(FactivaConfig).filter(FactivaConfig.id == 1).first()
   if not factiva_config or not factiva_config.enabled:
       raise HTTPException(status_code=503, detail="Factiva collection is disabled or not configured")
   query_params = {
       "industry_codes": factiva_config.industry_codes,
       "company_codes": factiva_config.company_codes,
       "keywords": factiva_config.keywords,
       "page_size": factiva_config.page_size,
   }
   ```

2. **Collect articles from Factiva:**
   ```python
   collector = FactivaCollector()
   if not collector.is_configured():
       raise HTTPException(status_code=503, detail="MMC API key not configured — cannot collect from Factiva")
   articles = collector.collect(query_params, run_id=run.id)
   ```

3. **URL deduplication (inline, fast):**
   Before semantic dedup, remove articles with duplicate `source_url`:
   ```python
   seen_urls = set()
   url_deduped = []
   for article in articles:
       url = article.get("source_url", "")
       if url and url in seen_urls:
           continue
       if url:
           seen_urls.add(url)
       url_deduped.append(article)
   articles = url_deduped
   ```
   Log: `logger.info(f"URL dedup: {len(articles_before)} -> {len(articles)}")`

4. **Semantic deduplication:**
   ```python
   deduplicator = ArticleDeduplicator()
   articles = deduplicator.deduplicate(articles)
   ```

5. **Load insurers for matching:**
   ```python
   insurers = db.query(Insurer).filter(
       Insurer.enabled == True,
       Insurer.category == request.category
   ).all()
   ```
   IMPORTANT: Filter by category so each run only matches against insurers in the requested category (Health, Dental, or Group Life). This is consistent with the existing per-category pipeline design.

6. **Ensure "General News" sentinel insurer exists for unmatched articles:**
   ```python
   general_insurer = db.query(Insurer).filter(Insurer.ans_code == "000000").first()
   if not general_insurer:
       general_insurer = Insurer(
           ans_code="000000",
           name="Noticias Gerais",
           category=request.category,
           enabled=True,
           search_terms=None,
       )
       db.add(general_insurer)
       db.commit()
       db.refresh(general_insurer)
   ```
   Use ANS code "000000" as the sentinel (real ANS codes are 6-digit non-zero).

7. **Match articles to insurers:**
   ```python
   matcher = InsurerMatcher()
   match_results = matcher.match_batch(articles, insurers)
   ```

8. **Store matched articles + classify:**
   ```python
   classifier = ClassificationService()
   items_stored = 0
   insurers_with_news = set()

   for article, match in zip(articles, match_results):
       # Determine insurer IDs — use sentinel for unmatched
       target_ids = match.insurer_ids if match.insurer_ids else [general_insurer.id]

       # Cap at 3 insurers per article to prevent runaway duplication
       target_ids = target_ids[:3]

       for insurer_id in target_ids:
           insurer = db.query(Insurer).filter(Insurer.id == insurer_id).first()
           insurer_name = insurer.name if insurer else "Unknown"

           # Classify
           classification = classifier.classify_single_news(
               insurer_name=insurer_name,
               news_title=article["title"],
               news_description=article.get("description"),
           )

           # Create NewsItem
           news_item = NewsItem(
               run_id=run.id,
               insurer_id=insurer_id,
               title=article["title"],
               description=article.get("description"),
               source_url=article.get("source_url"),
               source_name=article.get("source_name", "Factiva"),
               published_at=article.get("published_at"),
               status=classification.status if classification else None,
               sentiment=classification.sentiment if classification else None,
               summary="\n".join(classification.summary_bullets) if classification else None,
               category_indicators=",".join(classification.category_indicators) if classification and classification.category_indicators else None,
           )
           db.add(news_item)
           items_stored += 1
           insurers_with_news.add(insurer_id)

   db.commit()
   ```

9. **Critical alerts, reporting, and delivery** — keep the exact same logic as existing `_execute_category_run` (call `CriticalAlertService`, `_generate_and_send_report`, update run status). Copy the alert + report + run update block unchanged.

10. **Update run record:**
    ```python
    run.insurers_processed = len(insurers_with_news)
    run.items_found = items_stored
    ```

**Step 3: Update execute_run endpoint.**
Change the `execute_run` function body to always call `_execute_factiva_pipeline`:
```python
try:
    return await _execute_factiva_pipeline(request, run, db)
except Exception as e:
    ...
```
Remove the `process_all` branching — there is no distinction between single-insurer and category modes in the Factiva flow. The `insurer_id` field on ExecuteRequest is now ignored (Factiva collects all articles, then matches). Add a deprecation log if `insurer_id` is provided:
```python
if request.insurer_id:
    logger.warning(f"insurer_id parameter is deprecated in Factiva mode (was {request.insurer_id})")
```

**Step 4: Update execute_category_run endpoint.**
Same change — call `_execute_factiva_pipeline` internally.

**Step 5: Remove or replace scraper health endpoint.**
Replace the existing `/health/scraper` endpoint:
```python
@router.get("/health/scraper", tags=["Health"])
def scraper_health() -> dict:
    """Check health of Factiva news collection."""
    collector = FactivaCollector()
    return {
        "status": "ok" if collector.is_configured() else "unconfigured",
        "source": "factiva",
        "mmc_api_configured": collector.is_configured(),
    }
```

**Step 6: Keep all existing read-only endpoints unchanged.**
`list_runs`, `get_latest_runs`, `get_run_stats`, `get_run`, `get_run_news`, `get_run_delivery_status` — do NOT modify these.

**What NOT to do:**
- Do NOT remove the ExecuteRequest/ExecuteResponse/CategoryExecuteRequest models — admin dashboard still uses them
- Do NOT change the endpoint URLs or response schemas — scheduler and admin call these
- Do NOT remove the `_generate_and_send_report` helper — it is unchanged
- Do NOT touch email/report/alert logic — only the collection and matching steps change
  </action>
  <verify>
1. `python -c "from app.routers.runs import router; print('runs router imported successfully')"` succeeds
2. Verify no ScraperService references remain in active pipeline code:
   `python -c "import ast; tree = ast.parse(open('app/routers/runs.py').read()); names = [node.name for node in ast.walk(tree) if isinstance(node, ast.Name) and 'Scraper' in node.name]; assert not names, f'ScraperService still referenced: {names}'; print('No ScraperService references in active code')"` — this should pass.
   Note: If there is a comment mentioning ScraperService that is fine. The assertion checks AST Name nodes only.
3. Verify FactivaCollector is imported:
   `python -c "from app.routers import runs; assert hasattr(runs, 'FactivaCollector') or 'FactivaCollector' in open('app/routers/runs.py').read(); print('FactivaCollector present')"` succeeds
  </verify>
  <done>Pipeline in runs.py uses FactivaCollector for batch collection, ArticleDeduplicator for dedup, InsurerMatcher (with AI fallback) for insurer assignment. Each article creates one NewsItem per matched insurer. Unmatched articles go to "Noticias Gerais" sentinel. Apify ScraperService is no longer called. Health endpoint reports Factiva status.</done>
</task>

</tasks>

<verification>
- `python -c "from app.routers.runs import router"` succeeds with no import errors
- `grep -c "ScraperService" app/routers/runs.py` returns 0 (no references in active code)
- `grep -c "FactivaCollector" app/routers/runs.py` returns at least 2 (import + usage)
- `grep -c "InsurerMatcher" app/routers/runs.py` returns at least 2 (import + usage)
- `grep -c "ArticleDeduplicator" app/routers/runs.py` returns at least 2 (import + usage)
- All existing read-only endpoints (`/api/runs`, `/api/runs/latest`, `/api/runs/stats`, etc.) are unchanged
- Health endpoint returns Factiva status instead of scraper status
</verification>

<success_criteria>
- Pipeline flow: FactivaCollector.collect() -> URL dedup -> ArticleDeduplicator.deduplicate() -> InsurerMatcher.match_batch() -> NewsItem creation per matched insurer -> ClassificationService.classify_single_news() -> alerts + report + email
- No Apify/ScraperService in the active pipeline code path
- Unmatched articles stored with "Noticias Gerais" sentinel insurer (ANS 000000)
- Multi-insurer articles capped at 3 NewsItem rows per article
- Scheduler (which calls POST /api/runs/execute/category) automatically uses the new Factiva pipeline
- Application boots and runs router imports without error
</success_criteria>

<output>
After completion, create `.planning/phases/11-insurer-matching-pipeline/11-03-SUMMARY.md`
</output>
